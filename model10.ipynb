{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562d4287",
   "metadata": {},
   "source": [
    "##### DNN CNN\n",
    "    - 독립적인 정보\n",
    "    - 입력(x) 간의 순서나 연관성을 고려하지 않는다\n",
    "##### 시계열 : 시간의 연속적인 흐름\n",
    "    - 시계열 데이터 : 날씨, 주식, 문장 --> 순서가 중요한 데이터\n",
    "##### RNN \n",
    "    - 순환하는 구조\n",
    "        - 시점1 (월요일) : 맑음(x1)  정보가 RNN에 들어온다 -> RNN 날씨가 맑았음(h1) 이라는 요약본을 생성\n",
    "        - 시점2 (화요일) : 흐림(x2)  ->RNN 새로운정보(흐림, x2) + 어제의기억(맑았음, h1) 함께 고려\n",
    "            어제 맑았는데 오늘 흐림 h2 이라는 새로운 요약본을 생성\n",
    "        - 시점3 (수요일) : 비(x3) ->RNN 새로운정보(비, x3) 정보와 + 어제의기억(어제 맑았는데 오늘 흐림, h2) 새로운 상태 h3\n",
    "        - 반복 \n",
    "    - 알고리즘\n",
    "        - 각 시점(time step)에서  1. 현재의 입력 과 2. 과거의 기억(hidden state ht-1) 받아서 3. 현재의 결과물과 4. 다음 시점으로 넘겨줄 최신 기억 ht 을 생성\n",
    "        - ht 기억이 시계열 데이터의 맥락(Context) 저장하는 역할\n",
    "    - 장점\n",
    "        - 순서가 있는 데이터의 맥락을 학습\n",
    "    - 한계\n",
    "        - 기억력이 생각보다 짧다\n",
    "        - 시계열 데이터가 길어지면(예 100단계 전의 정보)\n",
    "        - 이전정보가 소실되거나 반대로 너무 강해져서 폭주가 되서 제대로 학습이 안된다\n",
    "        - 장기 기억 의존성 문제 (Long-Term Dependency Problem)\n",
    "##### LSTM & GRU\n",
    "    - LSTM(Long Short Term Memory) : RNN 내부에 게이트(Gate) 복잡한 장치 -> 잊고, 기억할 정보를 관리\n",
    "    - GRU(Gated Recurrent Unit) : LSTM 구조를 좀 더 단순화시킨 모델, LSTM성능은 비슷, 속도는 빠르다\n",
    "##### RNN 핵심수식\n",
    "    - 은닉상태계산\n",
    "        - h1 = tanh(wht-1 + wxt - bh)\n",
    "    - 출력계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/pia222sk20/python/refs/heads/main/data/time_data_train.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c164ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d76bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "class StockDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        url = 'https://raw.githubusercontent.com/pia222sk20/python/refs/heads/main/data/time_data_train.csv'\n",
    "        self.csv = pd.read_csv(url)\n",
    "        data = torch.Tensor(self.csv.iloc[: , 1: -1].values)\n",
    "        label = torch.Tensor(self.csv.iloc[: , -1].values.reshape(-1, 1))\n",
    "        self.data = StandardScaler().fit_transform(data)\n",
    "        # 정답이 숫자 크다면 정규화가 학습에 도움이 된다.\n",
    "        self.label = StandardScaler().fit_transform(label)\n",
    "        self.data = torch.Tensor(self.data)\n",
    "        self.label = torch.Tensor(self.label)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 30 # 사용가능한 배치 개수\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index: index + 30]\n",
    "        label = self.label[index + 30]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(StockDataSet()))\n",
    "data.size(), label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70498b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class StockRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockRNN, self).__init__()\n",
    "        # (30일, 배치 16개, 각 입력의 특성 4개) batch_first = False\n",
    "        # (배치 16개, 30일 각 입력의 특성 4개) batch_first = True\n",
    "        self.rnn = nn.RNN(input_size=4, hidden_size=8, num_layers=5, batch_first=True)\n",
    "        # 출력 (batch, 30, 8)\n",
    "        self.fc1 = nn.Linear(30*8, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, ho): # 입력데이터는 (16,30,4)\n",
    "        # ho = 초기 은닉 상태(num_layers, batch, hidden_size) (5,16,8)\n",
    "        # 출력 x 는 모든 시점에 대한 hidden output을 담고 있어야함 (batch, seq_len, hidden_size) (16,30,8)\n",
    "        # 출력 hn 최종은닉상태(각 레이어의 마지막 타임스탬프 hidden state) (num_layer, batch, hidden_state) (5,16,8)\n",
    "        x, hn = self.rnn(x, ho)\n",
    "        # mlp 입력으로 사용될 수 있도록 모양 변경\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        # mlp \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # 예측한 종가 1차원 벡터\n",
    "        out = torch.flatten(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = StockRNN()\n",
    "sample_data = torch.randn(16, 30, 4)\n",
    "# 초기 hidden state 값  (num_layer, batch_size, hidden_size) (5, 16, 8)\n",
    "ho = torch.zeros(5,16,8)\n",
    "out = rnn(sample_data, ho)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = StockRNN().to(device)\n",
    "dataset = StockDataSet()\n",
    "loader = DataLoader(dataset, batch_size=16)\n",
    "optim = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected hidden size (5,9,8) got [5,16,8]\n",
    "# Rnn이 처리하는 배치크기는 9, 우리가 설계한 ho 16\n",
    "print(len(dataset) % 16)\n",
    "# 마지막 배치 개수가 모자라서 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        optim.zero_grad() # 학습 다 끝냈는데 이전 기울기가 남아있는 상황 방지\n",
    "        # 초기 은닉 상태의 배치크기는 DataLoader가 주는 배치 크기\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        # 모델의 예측값\n",
    "        pred = model(data.to(device), ho)\n",
    "        # 손실값\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'epoch: {epoch+1} loss: {loss.item()}')\n",
    "torch.save(model.state_dict(), './rnn.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능평가하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loader = DataLoader(dataset,batch_size=1)\n",
    "preds = []\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load('rnn.pth', map_location=device,weights_only=False))\n",
    "    for data, label in loader:\n",
    "        # 초기 은닉상태의 배치크기는 DataLoader가 주는 배치 크기\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        # 모델의 예측값\n",
    "        pred = model(data.to(device), ho)\n",
    "        preds.append(pred.item())\n",
    "        # 손실값\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        total_loss += (loss.item() / len(loader))\n",
    "\n",
    "print(f'total_loss : {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d529c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(preds, label='preds')\n",
    "plt.plot(dataset.label, label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01151353",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_30 = df.iloc[-30:, 1:-1].values\n",
    "    X = torch.tensor(last_30, dtype=torch.float32).unsqueeze(0)\n",
    "    ho = torch.zeros(5,1,8).to(device)\n",
    "    pred = model(X.to(device), ho)\n",
    "    print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6becd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinance\n",
    "X = np.array([\n",
    "    [10,20],[100,200]\n",
    "])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d46ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0780b126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-29 00:00:00-04:00</th>\n",
       "      <td>424.810714</td>\n",
       "      <td>429.942203</td>\n",
       "      <td>422.627096</td>\n",
       "      <td>428.731293</td>\n",
       "      <td>17644100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-30 00:00:00-04:00</th>\n",
       "      <td>434.180393</td>\n",
       "      <td>435.232492</td>\n",
       "      <td>428.880188</td>\n",
       "      <td>429.306976</td>\n",
       "      <td>29749100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-31 00:00:00-04:00</th>\n",
       "      <td>412.264924</td>\n",
       "      <td>413.058981</td>\n",
       "      <td>403.272437</td>\n",
       "      <td>403.322083</td>\n",
       "      <td>53971000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-01 00:00:00-04:00</th>\n",
       "      <td>405.962252</td>\n",
       "      <td>412.403882</td>\n",
       "      <td>404.463494</td>\n",
       "      <td>407.312103</td>\n",
       "      <td>24230400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-04 00:00:00-05:00</th>\n",
       "      <td>406.746332</td>\n",
       "      <td>407.361738</td>\n",
       "      <td>402.547872</td>\n",
       "      <td>405.416321</td>\n",
       "      <td>19672300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2024-10-29 00:00:00-04:00  424.810714  429.942203  422.627096  428.731293   \n",
       "2024-10-30 00:00:00-04:00  434.180393  435.232492  428.880188  429.306976   \n",
       "2024-10-31 00:00:00-04:00  412.264924  413.058981  403.272437  403.322083   \n",
       "2024-11-01 00:00:00-04:00  405.962252  412.403882  404.463494  407.312103   \n",
       "2024-11-04 00:00:00-05:00  406.746332  407.361738  402.547872  405.416321   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2024-10-29 00:00:00-04:00  17644100        0.0           0.0  \n",
       "2024-10-30 00:00:00-04:00  29749100        0.0           0.0  \n",
       "2024-10-31 00:00:00-04:00  53971000        0.0           0.0  \n",
       "2024-11-01 00:00:00-04:00  24230400        0.0           0.0  \n",
       "2024-11-04 00:00:00-05:00  19672300        0.0           0.0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "dat = yf.Ticker(\"MSFT\")\n",
    "df = dat.history(period='1y')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df877b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수, 종속변수 분리\n",
    "# window size : 30(한달치 데이터를 하나의 dataset)\n",
    "# 배치는 3배치\n",
    "# 알고리즘은 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bc9ceba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "class YFieldDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.csv = df\n",
    "        # print(self.csv.iloc[: , : 4].values)\n",
    "        # print(self.csv.columns[:4])\n",
    "        data = torch.Tensor(self.csv.iloc[: , :4].values)\n",
    "        label = torch.Tensor(self.csv.iloc[: , 4].values.reshape(-1, 1))\n",
    "        self.data = StandardScaler().fit_transform(data)\n",
    "        # 정답이 숫자 크다면 정규화가 학습에 도움이 된다.\n",
    "        self.label = StandardScaler().fit_transform(label)\n",
    "        self.data = torch.Tensor(self.data)\n",
    "        self.label = torch.Tensor(self.label)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 30 # 사용가능한 배치 개수\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index: index + 30]\n",
    "        label = self.label[index + 30]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7a0e1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class YFieldRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YFieldRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=4, hidden_size=8, num_layers=5, batch_first=True)\n",
    "        self.fc1 = nn.Linear(30*8, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, ho): # 입력데이터는 (16,30,4)\n",
    "        x, hn = self.rnn(x, ho)\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        out = torch.flatten(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "417ac28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0744, -0.0761, -0.0689], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = YFieldRNN()\n",
    "sample_data = torch.randn(3, 30, 4)\n",
    "ho = torch.zeros(5,3,8)\n",
    "out = rnn(sample_data, ho)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "082b20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YFieldRNN().to(device)\n",
    "dataset = YFieldDataSet()\n",
    "loader = DataLoader(dataset, batch_size=3)\n",
    "optim = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9401c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/74 [00:00<?, ?it/s]c:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([3, 1])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "epoch: 1 loss: 0.343813419342041:  92%|█████████▏| 68/74 [00:01<00:00, 65.57it/s]  c:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "epoch: 1 loss: 1.3422856330871582: 100%|██████████| 74/74 [00:01<00:00, 63.63it/s]\n",
      "epoch: 2 loss: 1.4057421684265137: 100%|██████████| 74/74 [00:01<00:00, 60.11it/s]  \n",
      "epoch: 3 loss: 1.3042091131210327: 100%|██████████| 74/74 [00:01<00:00, 67.65it/s]  \n",
      "epoch: 4 loss: 1.368638515472412: 100%|██████████| 74/74 [00:01<00:00, 66.60it/s]   \n",
      "epoch: 5 loss: 1.3844778537750244: 100%|██████████| 74/74 [00:01<00:00, 62.47it/s]  \n",
      "epoch: 6 loss: 1.3738436698913574: 100%|██████████| 74/74 [00:01<00:00, 64.31it/s]  \n",
      "epoch: 7 loss: 1.364458441734314: 100%|██████████| 74/74 [00:01<00:00, 65.37it/s]   \n",
      "epoch: 8 loss: 1.354355812072754: 100%|██████████| 74/74 [00:01<00:00, 66.51it/s]   \n",
      "epoch: 9 loss: 1.3689959049224854: 100%|██████████| 74/74 [00:01<00:00, 68.35it/s]  \n",
      "epoch: 10 loss: 1.36799156665802: 100%|██████████| 74/74 [00:01<00:00, 43.79it/s]    \n",
      "epoch: 11 loss: 1.3683980703353882: 100%|██████████| 74/74 [00:01<00:00, 48.09it/s]  \n",
      "epoch: 12 loss: 1.3677446842193604: 100%|██████████| 74/74 [00:01<00:00, 43.69it/s]  \n",
      "epoch: 13 loss: 1.3671540021896362: 100%|██████████| 74/74 [00:01<00:00, 38.18it/s]  \n",
      "epoch: 14 loss: 1.3730443716049194: 100%|██████████| 74/74 [00:01<00:00, 46.25it/s]  \n",
      "epoch: 15 loss: 1.3751001358032227: 100%|██████████| 74/74 [00:01<00:00, 52.53it/s]  \n",
      "epoch: 16 loss: 1.376754641532898: 100%|██████████| 74/74 [00:01<00:00, 60.08it/s]   \n",
      "epoch: 17 loss: 1.378750205039978: 100%|██████████| 74/74 [00:01<00:00, 47.16it/s]   \n",
      "epoch: 18 loss: 1.3788504600524902: 100%|██████████| 74/74 [00:01<00:00, 47.91it/s]  \n",
      "epoch: 19 loss: 1.3780896663665771: 100%|██████████| 74/74 [00:01<00:00, 66.67it/s]  \n",
      "epoch: 20 loss: 1.3804265260696411: 100%|██████████| 74/74 [00:01<00:00, 69.23it/s]  \n",
      "epoch: 21 loss: 1.381279706954956: 100%|██████████| 74/74 [00:01<00:00, 62.91it/s]   \n",
      "epoch: 22 loss: 1.3793737888336182: 100%|██████████| 74/74 [00:01<00:00, 63.32it/s]  \n",
      "epoch: 23 loss: 1.3813844919204712: 100%|██████████| 74/74 [00:01<00:00, 71.13it/s]  \n",
      "epoch: 24 loss: 1.3793119192123413: 100%|██████████| 74/74 [00:01<00:00, 65.58it/s]  \n",
      "epoch: 25 loss: 1.3810473680496216: 100%|██████████| 74/74 [00:01<00:00, 62.37it/s]  \n",
      "epoch: 26 loss: 1.3807923793792725: 100%|██████████| 74/74 [00:01<00:00, 71.04it/s]  \n",
      "epoch: 27 loss: 1.3815419673919678: 100%|██████████| 74/74 [00:01<00:00, 66.20it/s]  \n",
      "epoch: 28 loss: 1.3806273937225342: 100%|██████████| 74/74 [00:01<00:00, 65.45it/s]  \n",
      "epoch: 29 loss: 1.3833189010620117: 100%|██████████| 74/74 [00:01<00:00, 65.17it/s]  \n",
      "epoch: 30 loss: 1.452513575553894: 100%|██████████| 74/74 [00:01<00:00, 70.33it/s]   \n",
      "epoch: 31 loss: 1.3713152408599854: 100%|██████████| 74/74 [00:01<00:00, 51.53it/s]  \n",
      "epoch: 32 loss: 1.3789571523666382: 100%|██████████| 74/74 [00:01<00:00, 44.99it/s]  \n",
      "epoch: 33 loss: 1.386349081993103: 100%|██████████| 74/74 [00:02<00:00, 31.62it/s]   \n",
      "epoch: 34 loss: 1.3932608366012573: 100%|██████████| 74/74 [00:01<00:00, 39.48it/s]  \n",
      "epoch: 35 loss: 1.3861124515533447: 100%|██████████| 74/74 [00:01<00:00, 49.51it/s]  \n",
      "epoch: 36 loss: 1.381490707397461: 100%|██████████| 74/74 [00:01<00:00, 54.33it/s]   \n",
      "epoch: 37 loss: 1.3830828666687012: 100%|██████████| 74/74 [00:01<00:00, 55.52it/s]  \n",
      "epoch: 38 loss: 1.3914443254470825: 100%|██████████| 74/74 [00:01<00:00, 38.44it/s]  \n",
      "epoch: 39 loss: 1.4177137613296509: 100%|██████████| 74/74 [00:02<00:00, 36.50it/s]  \n",
      "epoch: 40 loss: 1.4613088369369507: 100%|██████████| 74/74 [00:01<00:00, 39.42it/s]  \n",
      "epoch: 41 loss: 1.470153570175171: 100%|██████████| 74/74 [00:01<00:00, 41.26it/s]   \n",
      "epoch: 42 loss: 1.418428659439087: 100%|██████████| 74/74 [00:02<00:00, 31.98it/s]   \n",
      "epoch: 43 loss: 1.4824367761611938: 100%|██████████| 74/74 [00:02<00:00, 36.78it/s]  \n",
      "epoch: 44 loss: 1.3924983739852905: 100%|██████████| 74/74 [00:01<00:00, 43.19it/s]  \n",
      "epoch: 45 loss: 1.4006140232086182: 100%|██████████| 74/74 [00:01<00:00, 42.74it/s]  \n",
      "epoch: 46 loss: 1.3582391738891602: 100%|██████████| 74/74 [00:01<00:00, 46.26it/s]  \n",
      "epoch: 47 loss: 1.3735003471374512: 100%|██████████| 74/74 [00:01<00:00, 46.91it/s]   \n",
      "epoch: 48 loss: 1.376173973083496: 100%|██████████| 74/74 [00:01<00:00, 52.21it/s]   \n",
      "epoch: 49 loss: 1.33347749710083: 100%|██████████| 74/74 [00:01<00:00, 61.23it/s]    \n",
      "epoch: 50 loss: 1.3546202182769775: 100%|██████████| 74/74 [00:01<00:00, 66.47it/s]  \n",
      "epoch: 51 loss: 1.4963957071304321: 100%|██████████| 74/74 [00:01<00:00, 60.67it/s]  \n",
      "epoch: 52 loss: 1.3112576007843018: 100%|██████████| 74/74 [00:01<00:00, 70.55it/s]  \n",
      "epoch: 53 loss: 1.3037084341049194: 100%|██████████| 74/74 [00:01<00:00, 60.08it/s]  \n",
      "epoch: 54 loss: 1.2647641897201538: 100%|██████████| 74/74 [00:01<00:00, 56.10it/s]  \n",
      "epoch: 55 loss: 1.258928656578064: 100%|██████████| 74/74 [00:01<00:00, 63.34it/s]   \n",
      "epoch: 56 loss: 1.2929126024246216: 100%|██████████| 74/74 [00:01<00:00, 64.84it/s]  \n",
      "epoch: 57 loss: 1.2759448289871216: 100%|██████████| 74/74 [00:01<00:00, 55.75it/s]  \n",
      "epoch: 58 loss: 1.2645362615585327: 100%|██████████| 74/74 [00:01<00:00, 55.60it/s]  \n",
      "epoch: 59 loss: 1.31247079372406: 100%|██████████| 74/74 [00:01<00:00, 70.28it/s]    \n",
      "epoch: 60 loss: 1.3138092756271362: 100%|██████████| 74/74 [00:01<00:00, 65.93it/s]  \n",
      "epoch: 61 loss: 1.4028732776641846: 100%|██████████| 74/74 [00:01<00:00, 61.24it/s]  \n",
      "epoch: 62 loss: 1.1913436651229858: 100%|██████████| 74/74 [00:01<00:00, 66.90it/s]  \n",
      "epoch: 63 loss: 2.695190668106079: 100%|██████████| 74/74 [00:01<00:00, 60.50it/s]   \n",
      "epoch: 64 loss: 1.2776188850402832: 100%|██████████| 74/74 [00:01<00:00, 46.28it/s]  \n",
      "epoch: 65 loss: 1.7508528232574463: 100%|██████████| 74/74 [00:01<00:00, 43.87it/s]  \n",
      "epoch: 66 loss: 1.4435242414474487: 100%|██████████| 74/74 [00:01<00:00, 60.77it/s]  \n",
      "epoch: 67 loss: 1.4689090251922607: 100%|██████████| 74/74 [00:01<00:00, 66.96it/s]  \n",
      "epoch: 68 loss: 1.061644434928894: 100%|██████████| 74/74 [00:01<00:00, 70.76it/s]   \n",
      "epoch: 69 loss: 1.2554455995559692: 100%|██████████| 74/74 [00:01<00:00, 38.29it/s]  \n",
      "epoch: 70 loss: 1.2077373266220093: 100%|██████████| 74/74 [00:01<00:00, 51.17it/s]  \n",
      "epoch: 71 loss: 1.1560224294662476: 100%|██████████| 74/74 [00:01<00:00, 67.93it/s]  \n",
      "epoch: 72 loss: 1.1592122316360474: 100%|██████████| 74/74 [00:01<00:00, 55.45it/s]  \n",
      "epoch: 73 loss: 1.214881420135498: 100%|██████████| 74/74 [00:01<00:00, 64.51it/s]   \n",
      "epoch: 74 loss: 1.2765381336212158: 100%|██████████| 74/74 [00:01<00:00, 63.59it/s]  \n",
      "epoch: 75 loss: 1.1847602128982544: 100%|██████████| 74/74 [00:01<00:00, 62.20it/s]  \n",
      "epoch: 76 loss: 1.2528709173202515: 100%|██████████| 74/74 [00:01<00:00, 69.77it/s]  \n",
      "epoch: 77 loss: 1.240474820137024: 100%|██████████| 74/74 [00:01<00:00, 64.06it/s]   \n",
      "epoch: 78 loss: 1.1791990995407104: 100%|██████████| 74/74 [00:01<00:00, 66.72it/s]  \n",
      "epoch: 79 loss: 1.2393300533294678: 100%|██████████| 74/74 [00:01<00:00, 63.10it/s]  \n",
      "epoch: 80 loss: 1.289135456085205: 100%|██████████| 74/74 [00:01<00:00, 56.56it/s]   \n",
      "epoch: 81 loss: 1.2026467323303223: 100%|██████████| 74/74 [00:01<00:00, 64.85it/s]  \n",
      "epoch: 82 loss: 1.1467825174331665: 100%|██████████| 74/74 [00:01<00:00, 52.16it/s]  \n",
      "epoch: 83 loss: 1.2768224477767944: 100%|██████████| 74/74 [00:01<00:00, 63.86it/s]  \n",
      "epoch: 84 loss: 1.1068189144134521: 100%|██████████| 74/74 [00:01<00:00, 49.77it/s]  \n",
      "epoch: 85 loss: 1.291881799697876: 100%|██████████| 74/74 [00:01<00:00, 41.63it/s]   \n",
      "epoch: 86 loss: 1.0747570991516113: 100%|██████████| 74/74 [00:01<00:00, 38.78it/s]  \n",
      "epoch: 87 loss: 1.2331347465515137: 100%|██████████| 74/74 [00:01<00:00, 40.47it/s]  \n",
      "epoch: 88 loss: 1.0318409204483032: 100%|██████████| 74/74 [00:01<00:00, 42.62it/s]  \n",
      "epoch: 89 loss: 1.2065383195877075: 100%|██████████| 74/74 [00:01<00:00, 45.02it/s]  \n",
      "epoch: 90 loss: 1.0620038509368896: 100%|██████████| 74/74 [00:01<00:00, 42.03it/s]  \n",
      "epoch: 91 loss: 1.0750422477722168: 100%|██████████| 74/74 [00:01<00:00, 41.36it/s]  \n",
      "epoch: 92 loss: 1.3100371360778809: 100%|██████████| 74/74 [00:01<00:00, 44.76it/s]  \n",
      "epoch: 93 loss: 1.086418628692627: 100%|██████████| 74/74 [00:01<00:00, 41.02it/s]   \n",
      "epoch: 94 loss: 1.0694574117660522: 100%|██████████| 74/74 [00:01<00:00, 41.15it/s]  \n",
      "epoch: 95 loss: 1.0528209209442139: 100%|██████████| 74/74 [00:01<00:00, 48.21it/s]  \n",
      "epoch: 96 loss: 1.126055121421814: 100%|██████████| 74/74 [00:01<00:00, 41.91it/s]   \n",
      "epoch: 97 loss: 1.1260427236557007: 100%|██████████| 74/74 [00:01<00:00, 43.47it/s]  \n",
      "epoch: 98 loss: 1.1603866815567017: 100%|██████████| 74/74 [00:01<00:00, 45.82it/s]  \n",
      "epoch: 99 loss: 1.2083609104156494: 100%|██████████| 74/74 [00:01<00:00, 42.94it/s]  \n",
      "epoch: 100 loss: 1.0422762632369995: 100%|██████████| 74/74 [00:01<00:00, 45.92it/s]  \n",
      "epoch: 101 loss: 1.0147061347961426: 100%|██████████| 74/74 [00:01<00:00, 40.20it/s]  \n",
      "epoch: 102 loss: 0.9643124938011169: 100%|██████████| 74/74 [00:01<00:00, 44.66it/s]  \n",
      "epoch: 103 loss: 1.155746579170227: 100%|██████████| 74/74 [00:01<00:00, 48.95it/s]   \n",
      "epoch: 104 loss: 0.970430850982666: 100%|██████████| 74/74 [00:01<00:00, 45.97it/s]   \n",
      "epoch: 105 loss: 1.012068748474121: 100%|██████████| 74/74 [00:01<00:00, 47.75it/s]   \n",
      "epoch: 106 loss: 1.0217698812484741: 100%|██████████| 74/74 [00:01<00:00, 42.08it/s]  \n",
      "epoch: 107 loss: 0.8723604679107666: 100%|██████████| 74/74 [00:01<00:00, 42.12it/s]   \n",
      "epoch: 108 loss: 0.8639692068099976: 100%|██████████| 74/74 [00:01<00:00, 42.48it/s]  \n",
      "epoch: 109 loss: 1.0155911445617676: 100%|██████████| 74/74 [00:01<00:00, 43.33it/s]  \n",
      "epoch: 110 loss: 0.8865867853164673: 100%|██████████| 74/74 [00:01<00:00, 37.63it/s]  \n",
      "epoch: 111 loss: 0.9179438948631287: 100%|██████████| 74/74 [00:01<00:00, 43.01it/s]  \n",
      "epoch: 112 loss: 0.9928273558616638: 100%|██████████| 74/74 [00:01<00:00, 38.57it/s]  \n",
      "epoch: 113 loss: 0.8131492137908936: 100%|██████████| 74/74 [00:02<00:00, 36.77it/s]  \n",
      "epoch: 114 loss: 0.8682547807693481: 100%|██████████| 74/74 [00:01<00:00, 40.20it/s]  \n",
      "epoch: 115 loss: 1.2603168487548828: 100%|██████████| 74/74 [00:01<00:00, 42.85it/s]  \n",
      "epoch: 116 loss: 1.0070768594741821: 100%|██████████| 74/74 [00:01<00:00, 43.66it/s]  \n",
      "epoch: 117 loss: 0.9021978378295898: 100%|██████████| 74/74 [00:01<00:00, 41.08it/s]  \n",
      "epoch: 118 loss: 0.8206967115402222: 100%|██████████| 74/74 [00:01<00:00, 40.45it/s]  \n",
      "epoch: 119 loss: 0.9645905494689941: 100%|██████████| 74/74 [00:01<00:00, 42.73it/s]  \n",
      "epoch: 120 loss: 0.8163830637931824: 100%|██████████| 74/74 [00:01<00:00, 39.09it/s]  \n",
      "epoch: 121 loss: 0.8174079060554504: 100%|██████████| 74/74 [00:01<00:00, 46.33it/s]  \n",
      "epoch: 122 loss: 0.8244115710258484: 100%|██████████| 74/74 [00:02<00:00, 36.08it/s]  \n",
      "epoch: 123 loss: 0.6584344506263733: 100%|██████████| 74/74 [00:01<00:00, 45.90it/s]  \n",
      "epoch: 124 loss: 0.6868862509727478: 100%|██████████| 74/74 [00:01<00:00, 43.01it/s]  \n",
      "epoch: 125 loss: 0.6017969846725464: 100%|██████████| 74/74 [00:01<00:00, 46.57it/s]  \n",
      "epoch: 126 loss: 0.5702772736549377: 100%|██████████| 74/74 [00:01<00:00, 37.95it/s]  \n",
      "epoch: 127 loss: 0.6663482785224915: 100%|██████████| 74/74 [00:01<00:00, 38.10it/s]  \n",
      "epoch: 128 loss: 0.661821722984314: 100%|██████████| 74/74 [00:01<00:00, 42.60it/s]   \n",
      "epoch: 129 loss: 0.5660421252250671: 100%|██████████| 74/74 [00:01<00:00, 43.31it/s]  \n",
      "epoch: 130 loss: 0.598871648311615: 100%|██████████| 74/74 [00:01<00:00, 44.98it/s]   \n",
      "epoch: 131 loss: 0.48577988147735596: 100%|██████████| 74/74 [00:01<00:00, 44.11it/s] \n",
      "epoch: 132 loss: 0.5804185271263123: 100%|██████████| 74/74 [00:01<00:00, 44.55it/s]  \n",
      "epoch: 133 loss: 0.4484439194202423: 100%|██████████| 74/74 [00:01<00:00, 40.18it/s]  \n",
      "epoch: 134 loss: 0.7396650314331055: 100%|██████████| 74/74 [00:01<00:00, 42.06it/s]  \n",
      "epoch: 135 loss: 0.7626180648803711: 100%|██████████| 74/74 [00:02<00:00, 34.67it/s]  \n",
      "epoch: 136 loss: 0.6667673587799072: 100%|██████████| 74/74 [00:01<00:00, 39.01it/s]  \n",
      "epoch: 137 loss: 0.5299939513206482: 100%|██████████| 74/74 [00:01<00:00, 39.32it/s]  \n",
      "epoch: 138 loss: 0.5140938758850098: 100%|██████████| 74/74 [00:01<00:00, 38.51it/s]  \n",
      "epoch: 139 loss: 0.41685864329338074: 100%|██████████| 74/74 [00:01<00:00, 43.70it/s] \n",
      "epoch: 140 loss: 0.4278980493545532: 100%|██████████| 74/74 [00:01<00:00, 46.50it/s]  \n",
      "epoch: 141 loss: 0.3536923825740814: 100%|██████████| 74/74 [00:01<00:00, 41.31it/s]  \n",
      "epoch: 142 loss: 0.2678491175174713: 100%|██████████| 74/74 [00:01<00:00, 44.68it/s]  \n",
      "epoch: 143 loss: 0.45959052443504333: 100%|██████████| 74/74 [00:01<00:00, 41.22it/s]  \n",
      "epoch: 144 loss: 0.2204989343881607: 100%|██████████| 74/74 [00:01<00:00, 42.60it/s]  \n",
      "epoch: 145 loss: 0.22071573138237: 100%|██████████| 74/74 [00:01<00:00, 47.08it/s]    \n",
      "epoch: 146 loss: 0.26019442081451416: 100%|██████████| 74/74 [00:01<00:00, 44.94it/s] \n",
      "epoch: 147 loss: 0.20103168487548828: 100%|██████████| 74/74 [00:01<00:00, 41.84it/s] \n",
      "epoch: 148 loss: 0.19809550046920776: 100%|██████████| 74/74 [00:01<00:00, 45.45it/s] \n",
      "epoch: 149 loss: 0.24285274744033813: 100%|██████████| 74/74 [00:01<00:00, 48.34it/s] \n",
      "epoch: 150 loss: 0.23235410451889038: 100%|██████████| 74/74 [00:01<00:00, 49.37it/s] \n",
      "epoch: 151 loss: 0.27136850357055664: 100%|██████████| 74/74 [00:01<00:00, 41.40it/s] \n",
      "epoch: 152 loss: 0.28562647104263306: 100%|██████████| 74/74 [00:01<00:00, 44.99it/s] \n",
      "epoch: 153 loss: 0.4210384488105774: 100%|██████████| 74/74 [00:01<00:00, 39.33it/s]  \n",
      "epoch: 154 loss: 0.23773562908172607: 100%|██████████| 74/74 [00:02<00:00, 33.85it/s] \n",
      "epoch: 155 loss: 0.15843716263771057: 100%|██████████| 74/74 [00:01<00:00, 38.62it/s] \n",
      "epoch: 156 loss: 0.33625057339668274: 100%|██████████| 74/74 [00:01<00:00, 39.84it/s] \n",
      "epoch: 157 loss: 0.11735224723815918: 100%|██████████| 74/74 [00:01<00:00, 40.92it/s] \n",
      "epoch: 158 loss: 0.24413257837295532: 100%|██████████| 74/74 [00:01<00:00, 44.11it/s] \n",
      "epoch: 159 loss: 0.10160009562969208: 100%|██████████| 74/74 [00:01<00:00, 38.00it/s] \n",
      "epoch: 160 loss: 0.15406839549541473: 100%|██████████| 74/74 [00:01<00:00, 46.66it/s] \n",
      "epoch: 161 loss: 0.16061994433403015: 100%|██████████| 74/74 [00:01<00:00, 42.06it/s] \n",
      "epoch: 162 loss: 0.07693181931972504: 100%|██████████| 74/74 [00:02<00:00, 36.38it/s] \n",
      "epoch: 163 loss: 0.16621029376983643: 100%|██████████| 74/74 [00:01<00:00, 40.37it/s] \n",
      "epoch: 164 loss: 0.055594850331544876: 100%|██████████| 74/74 [00:01<00:00, 47.00it/s]\n",
      "epoch: 165 loss: 0.03723566234111786: 100%|██████████| 74/74 [00:02<00:00, 35.51it/s] \n",
      "epoch: 166 loss: 0.05498110502958298: 100%|██████████| 74/74 [00:01<00:00, 47.63it/s] \n",
      "epoch: 167 loss: 0.03831663727760315: 100%|██████████| 74/74 [00:01<00:00, 38.09it/s] \n",
      "epoch: 168 loss: 0.015430804342031479: 100%|██████████| 74/74 [00:01<00:00, 42.16it/s]\n",
      "epoch: 169 loss: 0.15341816842556: 100%|██████████| 74/74 [00:01<00:00, 45.41it/s]    \n",
      "epoch: 170 loss: 0.04371825233101845: 100%|██████████| 74/74 [00:01<00:00, 37.65it/s] \n",
      "epoch: 171 loss: 0.023636937141418457: 100%|██████████| 74/74 [00:01<00:00, 42.29it/s]\n",
      "epoch: 172 loss: 0.09274779260158539: 100%|██████████| 74/74 [00:01<00:00, 43.19it/s] \n",
      "epoch: 173 loss: 0.03897416219115257: 100%|██████████| 74/74 [00:01<00:00, 41.53it/s] \n",
      "epoch: 174 loss: 0.052076444029808044: 100%|██████████| 74/74 [00:01<00:00, 45.77it/s]\n",
      "epoch: 175 loss: 0.04420948401093483: 100%|██████████| 74/74 [00:01<00:00, 44.39it/s] \n",
      "epoch: 176 loss: 0.05381321161985397: 100%|██████████| 74/74 [00:01<00:00, 40.69it/s] \n",
      "epoch: 177 loss: 0.026608552783727646: 100%|██████████| 74/74 [00:01<00:00, 37.39it/s]\n",
      "epoch: 178 loss: 0.03917500376701355: 100%|██████████| 74/74 [00:01<00:00, 43.01it/s] \n",
      "epoch: 179 loss: 0.0505688339471817: 100%|██████████| 74/74 [00:01<00:00, 41.90it/s]  \n",
      "epoch: 180 loss: 0.015598442405462265: 100%|██████████| 74/74 [00:01<00:00, 42.69it/s]\n",
      "epoch: 181 loss: 0.13458864390850067: 100%|██████████| 74/74 [00:01<00:00, 42.26it/s] \n",
      "epoch: 182 loss: 0.025689369067549706: 100%|██████████| 74/74 [00:01<00:00, 47.74it/s]\n",
      "epoch: 183 loss: 0.08457291126251221: 100%|██████████| 74/74 [00:01<00:00, 41.79it/s] \n",
      "epoch: 184 loss: 0.00684346305206418: 100%|██████████| 74/74 [00:01<00:00, 41.04it/s] \n",
      "epoch: 185 loss: 0.09095586091279984: 100%|██████████| 74/74 [00:01<00:00, 46.94it/s] \n",
      "epoch: 186 loss: 0.200041264295578: 100%|██████████| 74/74 [00:01<00:00, 41.08it/s]   \n",
      "epoch: 187 loss: 0.00025654875207692385: 100%|██████████| 74/74 [00:01<00:00, 37.60it/s]\n",
      "epoch: 188 loss: 0.00033900371636264026: 100%|██████████| 74/74 [00:01<00:00, 40.91it/s]\n",
      "epoch: 189 loss: 0.01092913281172514: 100%|██████████| 74/74 [00:02<00:00, 35.89it/s] \n",
      "epoch: 190 loss: 0.040301207453012466: 100%|██████████| 74/74 [00:01<00:00, 40.91it/s]\n",
      "epoch: 191 loss: 0.0025282136630266905: 100%|██████████| 74/74 [00:01<00:00, 45.99it/s]\n",
      "epoch: 192 loss: 0.35058948397636414: 100%|██████████| 74/74 [00:01<00:00, 43.63it/s] \n",
      "epoch: 193 loss: 0.06556486338376999: 100%|██████████| 74/74 [00:01<00:00, 47.58it/s] \n",
      "epoch: 194 loss: 0.005646673496812582: 100%|██████████| 74/74 [00:01<00:00, 44.01it/s]\n",
      "epoch: 195 loss: 0.0021085545886307955: 100%|██████████| 74/74 [00:01<00:00, 38.97it/s]\n",
      "epoch: 196 loss: 0.02105257287621498: 100%|██████████| 74/74 [00:01<00:00, 47.27it/s] \n",
      "epoch: 197 loss: 0.11529352515935898: 100%|██████████| 74/74 [03:59<00:00,  3.23s/it] \n",
      "epoch: 198 loss: 0.017499731853604317: 100%|██████████| 74/74 [00:02<00:00, 33.18it/s]\n",
      "epoch: 199 loss: 0.0031208505388349295: 100%|██████████| 74/74 [00:01<00:00, 44.38it/s]\n",
      "epoch: 200 loss: 0.001575491507537663: 100%|██████████| 74/74 [00:01<00:00, 49.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        optim.zero_grad() # 학습 다 끝냈는데 이전 기울기가 남아있는 상황 방지\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        pred = model(data.to(device), ho)\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'epoch: {epoch+1} loss: {loss.item()}')\n",
    "torch.save(model.state_dict(), './yfinance.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eae6ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss : 0.5469\n"
     ]
    }
   ],
   "source": [
    "# 모델 성능평가하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loader = DataLoader(dataset,batch_size=1)\n",
    "preds = []\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load('yfinance.pth', map_location=device,weights_only=False))\n",
    "    for data, label in loader:\n",
    "        # 초기 은닉상태의 배치크기는 DataLoader가 주는 배치 크기\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        # 모델의 예측값\n",
    "        pred = model(data.to(device), ho)\n",
    "        preds.append(pred.item())\n",
    "        # 손실값\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        total_loss += (loss.item() / len(loader))\n",
    "\n",
    "print(f'total_loss : {total_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
