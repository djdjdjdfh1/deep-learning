{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562d4287",
   "metadata": {},
   "source": [
    "##### DNN CNN\n",
    "    - 독립적인 정보\n",
    "    - 입력(x) 간의 순서나 연관성을 고려하지 않는다\n",
    "##### 시계열 : 시간의 연속적인 흐름\n",
    "    - 시계열 데이터 : 날씨, 주식, 문장 --> 순서가 중요한 데이터\n",
    "##### RNN \n",
    "    - 순환하는 구조\n",
    "        - 시점1 (월요일) : 맑음(x1)  정보가 RNN에 들어온다 -> RNN 날씨가 맑았음(h1) 이라는 요약본을 생성\n",
    "        - 시점2 (화요일) : 흐림(x2)  ->RNN 새로운정보(흐림, x2) + 어제의기억(맑았음, h1) 함께 고려\n",
    "            어제 맑았는데 오늘 흐림 h2 이라는 새로운 요약본을 생성\n",
    "        - 시점3 (수요일) : 비(x3) ->RNN 새로운정보(비, x3) 정보와 + 어제의기억(어제 맑았는데 오늘 흐림, h2) 새로운 상태 h3\n",
    "        - 반복 \n",
    "    - 알고리즘\n",
    "        - 각 시점(time step)에서  1. 현재의 입력 과 2. 과거의 기억(hidden state ht-1) 받아서 3. 현재의 결과물과 4. 다음 시점으로 넘겨줄 최신 기억 ht 을 생성\n",
    "        - ht 기억이 시계열 데이터의 맥락(Context) 저장하는 역할\n",
    "    - 장점\n",
    "        - 순서가 있는 데이터의 맥락을 학습\n",
    "    - 한계\n",
    "        - 기억력이 생각보다 짧다\n",
    "        - 시계열 데이터가 길어지면(예 100단계 전의 정보)\n",
    "        - 이전정보가 소실되거나 반대로 너무 강해져서 폭주가 되서 제대로 학습이 안된다\n",
    "        - 장기 기억 의존성 문제 (Long-Term Dependency Problem)\n",
    "##### LSTM & GRU\n",
    "    - LSTM(Long Short Term Memory) : RNN 내부에 게이트(Gate) 복잡한 장치 -> 잊고, 기억할 정보를 관리\n",
    "    - GRU(Gated Recurrent Unit) : LSTM 구조를 좀 더 단순화시킨 모델, LSTM성능은 비슷, 속도는 빠르다\n",
    "##### RNN 핵심수식\n",
    "    - 은닉상태계산\n",
    "        - h1 = tanh(wht-1 + wxt - bh)\n",
    "    - 출력계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fa8180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>120</td>\n",
       "      <td>123</td>\n",
       "      <td>118</td>\n",
       "      <td>13181000</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-17</td>\n",
       "      <td>124</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>17284900</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-18</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>118</td>\n",
       "      <td>17948100</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-21</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>11670000</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-22</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>115</td>\n",
       "      <td>9689000</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open  High  Low    Volume  Close\n",
       "0  2015-12-16   120   123  118  13181000    123\n",
       "1  2015-12-17   124   126  122  17284900    123\n",
       "2  2015-12-18   121   122  118  17948100    118\n",
       "3  2015-12-21   120   120  116  11670000    117\n",
       "4  2015-12-22   117   117  115   9689000    116"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/pia222sk20/python/refs/heads/main/data/time_data_train.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12c164ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 967 entries, 0 to 966\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    967 non-null    object\n",
      " 1   Open    967 non-null    int64 \n",
      " 2   High    967 non-null    int64 \n",
      " 3   Low     967 non-null    int64 \n",
      " 4   Volume  967 non-null    int64 \n",
      " 5   Close   967 non-null    int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 45.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e74c22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49d76bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "class StockDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        url = 'https://raw.githubusercontent.com/pia222sk20/python/refs/heads/main/data/time_data_train.csv'\n",
    "        self.csv = pd.read_csv(url)\n",
    "        data = torch.Tensor(self.csv.iloc[: , 1: -1].values)\n",
    "        label = torch.Tensor(self.csv.iloc[: , -1].values.reshape(-1, 1))\n",
    "        self.data = StandardScaler().fit_transform(data)\n",
    "        # 정답이 숫자 크다면 정규화가 학습에 도움이 된다.\n",
    "        self.label = StandardScaler().fit_transform(label)\n",
    "        self.data = torch.Tensor(self.data)\n",
    "        self.label = torch.Tensor(self.label)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 30 # 사용가능한 배치 개수\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index: index + 30]\n",
    "        label = self.label[index + 30]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e0d65eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 4]), torch.Size([1]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = next(iter(StockDataSet()))\n",
    "data.size(), label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70498b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class StockRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockRNN, self).__init__()\n",
    "        # (30일, 배치 16개, 각 입력의 특성 4개) batch_first = False\n",
    "        # (배치 16개, 30일 각 입력의 특성 4개) batch_first = True\n",
    "        self.rnn = nn.RNN(input_size=4, hidden_size=8, num_layers=5, batch_first=True)\n",
    "        # 출력 (batch, 30, 8)\n",
    "        self.fc1 = nn.Linear(30*8, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, ho): # 입력데이터는 (16,30,4)\n",
    "        # ho = 초기 은닉 상태(num_layers, batch, hidden_size) (5,16,8)\n",
    "        # 출력 x 는 모든 시점에 대한 hidden output을 담고 있어야함 (batch, seq_len, hidden_size) (16,30,8)\n",
    "        # 출력 hn 최종은닉상태(각 레이어의 마지막 타임스탬프 hidden state) (num_layer, batch, hidden_state) (5,16,8)\n",
    "        x, hn = self.rnn(x, ho)\n",
    "        # mlp 입력으로 사용될 수 있도록 모양 변경\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        # mlp \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # 예측한 종가 1차원 벡터\n",
    "        out = torch.flatten(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4a6b928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0556, -0.0638, -0.0643, -0.0819, -0.0552, -0.0634, -0.0580, -0.0569,\n",
       "        -0.0756, -0.0825, -0.0652, -0.0850, -0.0836, -0.0699, -0.0834, -0.0736],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = StockRNN()\n",
    "sample_data = torch.randn(16, 30, 4)\n",
    "# 초기 hidden state 값  (num_layer, batch_size, hidden_size) (5, 16, 8)\n",
    "ho = torch.zeros(5,16,8)\n",
    "out = rnn(sample_data, ho)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abd4aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = StockRNN().to(device)\n",
    "dataset = StockDataSet()\n",
    "loader = DataLoader(dataset, batch_size=16)\n",
    "optim = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1892d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Expected hidden size (5,9,8) got [5,16,8]\n",
    "# Rnn이 처리하는 배치크기는 9, 우리가 설계한 ho 16\n",
    "print(len(dataset) % 16)\n",
    "# 마지막 배치 개수가 모자라서 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364e039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]c:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "epoch: 1 loss: 0.7504280209541321:  98%|█████████▊| 58/59 [00:01<00:00, 56.83it/s]   \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (5, 9, 8), got [5, 16, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m ho = torch.zeros(\u001b[32m5\u001b[39m,\u001b[32m16\u001b[39m,\u001b[32m8\u001b[39m).to(device)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 모델의 예측값\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 손실값\u001b[39;00m\n\u001b[32m     11\u001b[39m loss = nn.MSELoss()(pred, label.to(device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mStockRNN.forward\u001b[39m\u001b[34m(self, x, ho)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, ho): \u001b[38;5;66;03m# 입력데이터는 (16,30,4)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# ho = 초기 은닉 상태(num_layers, batch, hidden_size) (5,16,8)\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# 출력 x 는 모든 시점에 대한 hidden output을 담고 있어야함 (batch, seq_len, hidden_size) (16,30,8)\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# 출력 hn 최종은닉상태(각 레이어의 마지막 타임스탬프 hidden state) (num_layer, batch, hidden_state) (5,16,8)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     x, hn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# mlp 입력으로 사용될 수 있도록 모양 변경\u001b[39;00m\n\u001b[32m     18\u001b[39m     x = torch.reshape(x, (x.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:710\u001b[39m, in \u001b[36mRNN.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m    707\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mRNN_TANH\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mRNN_RELU\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:364\u001b[39m, in \u001b[36mRNNBase.check_forward_args\u001b[39m\u001b[34m(self, input, hidden, batch_sizes)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28mself\u001b[39m.check_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[32m    362\u001b[39m expected_hidden_size = \u001b[38;5;28mself\u001b[39m.get_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hidden_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:345\u001b[39m, in \u001b[36mRNNBase.check_hidden_size\u001b[39m\u001b[34m(self, hx, expected_hidden_size, msg)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_hidden_size\u001b[39m(\n\u001b[32m    339\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    340\u001b[39m     hx: Tensor,\n\u001b[32m    341\u001b[39m     expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[32m    342\u001b[39m     msg: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    343\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hx.size() != expected_hidden_size:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg.format(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx.size())))\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected hidden size (5, 9, 8), got [5, 16, 8]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        optim.zero_grad() # 학습 다 끝냈는데 이전 기울기가 남아있는 상황 방지\n",
    "        # 초기 은닉 상태의 배치크기는 DataLoader가 주는 배치 크기\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        # 모델의 예측값\n",
    "        pred = model(data.to(device), ho)\n",
    "        # 손실값\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'epoch: {epoch+1} loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
